{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87aa18af-3840-4018-a788-c6d193799b54",
      "metadata": {
        "id": "87aa18af-3840-4018-a788-c6d193799b54"
      },
      "outputs": [],
      "source": [
        "!pip install transformers sentencepiece datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8erPdx2JgYAS",
      "metadata": {
        "id": "8erPdx2JgYAS"
      },
      "outputs": [],
      "source": [
        "pip install accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ek_OlvEqlfwM",
      "metadata": {
        "id": "ek_OlvEqlfwM"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoConfig, AutoModelForSequenceClassification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uG2dj2JZTLuf",
      "metadata": {
        "id": "uG2dj2JZTLuf"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "S_odkf81S2dB",
      "metadata": {
        "id": "S_odkf81S2dB"
      },
      "outputs": [],
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "#bigscience/bloomz-560m\n",
        "#bert-base-uncased\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdda9392-0bfd-419d-bd3c-3b305fd6fa35",
      "metadata": {
        "id": "fdda9392-0bfd-419d-bd3c-3b305fd6fa35"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"glue\", \"sst2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "952b9693-f642-4cdc-b904-071358342310",
      "metadata": {
        "id": "952b9693-f642-4cdc-b904-071358342310",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "model_name = \"bert-base-uncased\"\n",
        "#tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "#tokenizer.pad_token = tokenizer.eos_token\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26da3d5d-52e4-4b19-965d-3abf49ddb02b",
      "metadata": {
        "id": "26da3d5d-52e4-4b19-965d-3abf49ddb02b"
      },
      "outputs": [],
      "source": [
        "\n",
        "#from bertlt import  PrefixForSequenceClassification, PromptForSequenceClassification\n",
        "from transformers import AutoConfig\n",
        "config = AutoConfig.from_pretrained(\"Kowsher/L-Tuning\")\n",
        "config._name_or_path=model_name\n",
        "config.hidden_size=768\n",
        "config.num_hidden_layers=12\n",
        "config.n_head=12\n",
        "config.num_labels=2\n",
        "config.pad_token_id=tokenizer.pad_token_id\n",
        "config.hidden_dropout = 0.01\n",
        "config.model_type='bert'\n",
        "config.cls_token_id=tokenizer.cls_token_id\n",
        "config.sep_token_id=tokenizer.sep_token_id\n",
        "config.pooling=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7de8ed68-2bfe-4768-884b-db086e0b1a26",
      "metadata": {
        "id": "7de8ed68-2bfe-4768-884b-db086e0b1a26"
      },
      "outputs": [],
      "source": [
        "b = AutoConfig.from_pretrained(model_name)\n",
        "b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fe567bc-af62-4750-b6a4-40b33e766842",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fe567bc-af62-4750-b6a4-40b33e766842",
        "outputId": "4ea7ea2b-2357-473d-e18c-4efcd4a05985"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prefix c (pre_seq_len) is 10\n"
          ]
        }
      ],
      "source": [
        "from torch import nn\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "import torch\n",
        "import random\n",
        "from DataCollator import DataCollatorForLTCls\n",
        "# define the label correctly from the dataset\n",
        "label2prompt = {0: 'The text has negative sentiment, text: ',  1: 'The text has positive sentiment, text: '}\n",
        "\n",
        "data_collator = DataCollatorForLTCls(config, tokenizer, label2prompt)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80565bcb-1547-4eb9-84ee-e914f98adf72",
      "metadata": {
        "id": "80565bcb-1547-4eb9-84ee-e914f98adf72"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa388ef6-4f85-4094-8675-1321ac3faf18",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa388ef6-4f85-4094-8675-1321ac3faf18",
        "outputId": "ddae6d55-1b39-4e6d-8ed5-223e7a6d5fd3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of PromptForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['prompt_encoder.W.weight', 'prompt_encoder.W.bias', 'classifier.weight', 'prompt_encoder.projection.bias', 'classifier.bias', 'prompt_encoder.projection.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from bertlt import PromptForSequenceClassification\n",
        "config.pre_seq_len=10\n",
        "\n",
        "model = PromptForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    config=config,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1429167a-3b74-4686-bccc-ea7804e6cad0",
      "metadata": {
        "id": "1429167a-3b74-4686-bccc-ea7804e6cad0",
        "outputId": "e172eea1-b3c0-44f3-d2f8-98b4a8b3f8a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Parameters: 109569027\n",
            "Trainable Parameters: 86787\n",
            "Percentage Trainable: 0.07920760307563924629%\n"
          ]
        }
      ],
      "source": [
        "# Total number of parameters in the model\n",
        "total_parameters = model.num_parameters()\n",
        "\n",
        "# Total number of trainable parameters in the model\n",
        "trainable_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "# Calculate the percentage of trainable parameters\n",
        "percentage_trainable = (trainable_parameters / total_parameters) * 100\n",
        "\n",
        "print(f\"Total Parameters: {total_parameters}\")\n",
        "print(f\"Trainable Parameters: {trainable_parameters}\")\n",
        "print(f\"Percentage Trainable: {percentage_trainable:.20f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "680aa280-2e2f-410a-882b-cb016e938368",
      "metadata": {
        "id": "680aa280-2e2f-410a-882b-cb016e938368"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d912acf-1b7d-4bb2-9bd6-977639168580",
      "metadata": {
        "id": "3d912acf-1b7d-4bb2-9bd6-977639168580",
        "outputId": "ba53ee56-72b6-44cf-a404-523d24e430eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /home/kowsher/miniconda3/envs/LT/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /home/kowsher/miniconda3/envs/LT/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/kowsher/miniconda3/envs/LT/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/kowsher/miniconda3/envs/LT did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/home/kowsher/miniconda3/envs/LT/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('@/tmp/.ICE-unix/1423,unix/kowsher-XPS-8950'), PosixPath('local/kowsher-XPS-8950')}\n",
            "  warn(msg)\n",
            "/home/kowsher/miniconda3/envs/LT/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/etc/xdg/xdg-ubuntu')}\n",
            "  warn(msg)\n",
            "/home/kowsher/miniconda3/envs/LT/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('1'), PosixPath('0')}\n",
            "  warn(msg)\n",
            "/home/kowsher/miniconda3/envs/LT/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/org/gnome/Terminal/screen/d5ea2a9b_bcca_41aa_bc1c_68630b6c7485')}\n",
            "  warn(msg)\n",
            "/home/kowsher/miniconda3/envs/LT/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
            "  warn(msg)\n",
            "/home/kowsher/miniconda3/envs/LT/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import EarlyStoppingCallback, IntervalStrategy\n",
        "from sklearn.metrics import r2_score, accuracy_score, matthews_corrcoef\n",
        "import numpy as np\n",
        "\n",
        "def compute_metrics(p):\n",
        "    logits = p.predictions\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    labels = p.label_ids\n",
        "\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "\n",
        "\n",
        "\n",
        "    return {\"acc\": accuracy}\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./rfalcon_task',\n",
        "    num_train_epochs=10,\n",
        "    do_eval=True,\n",
        "    #learning_rate=0.001,\n",
        "    #bf16=True,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "\n",
        "    logging_dir='./logs',\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=100,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps = 100,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=100,\n",
        "\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    #optim=\"paged_adamw_8bit\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d6a7bf2-41e2-4be0-beaf-07a7d1f48112",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 623
        },
        "id": "5d6a7bf2-41e2-4be0-beaf-07a7d1f48112",
        "outputId": "13205143-d5d3-4bea-e8af-973bb1da2c2b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3400' max='21050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 3400/21050 18:13 < 1:34:41, 3.11 it/s, Epoch 1/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.299400</td>\n",
              "      <td>0.319096</td>\n",
              "      <td>0.873853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.332400</td>\n",
              "      <td>0.303545</td>\n",
              "      <td>0.875000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.302600</td>\n",
              "      <td>0.308947</td>\n",
              "      <td>0.877294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.323600</td>\n",
              "      <td>0.296372</td>\n",
              "      <td>0.876147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.299700</td>\n",
              "      <td>0.303932</td>\n",
              "      <td>0.869266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.309800</td>\n",
              "      <td>0.303495</td>\n",
              "      <td>0.873853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.329100</td>\n",
              "      <td>0.295724</td>\n",
              "      <td>0.873853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.311100</td>\n",
              "      <td>0.301468</td>\n",
              "      <td>0.875000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.326500</td>\n",
              "      <td>0.296935</td>\n",
              "      <td>0.876147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.303800</td>\n",
              "      <td>0.308704</td>\n",
              "      <td>0.873853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.305200</td>\n",
              "      <td>0.302153</td>\n",
              "      <td>0.873853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.316400</td>\n",
              "      <td>0.301461</td>\n",
              "      <td>0.878440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.314800</td>\n",
              "      <td>0.308478</td>\n",
              "      <td>0.875000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.303900</td>\n",
              "      <td>0.294087</td>\n",
              "      <td>0.883028</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.307800</td>\n",
              "      <td>0.297338</td>\n",
              "      <td>0.879587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.314600</td>\n",
              "      <td>0.301787</td>\n",
              "      <td>0.877294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.314100</td>\n",
              "      <td>0.297721</td>\n",
              "      <td>0.875000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.308900</td>\n",
              "      <td>0.309261</td>\n",
              "      <td>0.880734</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>0.297800</td>\n",
              "      <td>0.305433</td>\n",
              "      <td>0.872706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.302800</td>\n",
              "      <td>0.312848</td>\n",
              "      <td>0.876147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>0.334400</td>\n",
              "      <td>0.301502</td>\n",
              "      <td>0.876147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>0.320900</td>\n",
              "      <td>0.299006</td>\n",
              "      <td>0.871560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>0.305300</td>\n",
              "      <td>0.311359</td>\n",
              "      <td>0.876147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>0.310000</td>\n",
              "      <td>0.308494</td>\n",
              "      <td>0.878440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.305100</td>\n",
              "      <td>0.299681</td>\n",
              "      <td>0.878440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>0.296000</td>\n",
              "      <td>0.303934</td>\n",
              "      <td>0.873853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>0.314600</td>\n",
              "      <td>0.302205</td>\n",
              "      <td>0.873853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>0.286000</td>\n",
              "      <td>0.286799</td>\n",
              "      <td>0.905000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2900</td>\n",
              "      <td>0.309800</td>\n",
              "      <td>0.298848</td>\n",
              "      <td>0.878440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.280400</td>\n",
              "      <td>0.288050</td>\n",
              "      <td>0.903028</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3100</td>\n",
              "      <td>0.307000</td>\n",
              "      <td>0.295025</td>\n",
              "      <td>0.876147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3200</td>\n",
              "      <td>0.317500</td>\n",
              "      <td>0.309813</td>\n",
              "      <td>0.876147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3300</td>\n",
              "      <td>0.302600</td>\n",
              "      <td>0.299105</td>\n",
              "      <td>0.880734</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3400</td>\n",
              "      <td>0.291100</td>\n",
              "      <td>0.292311</td>\n",
              "      <td>0.893853</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=3400, training_loss=0.28101540195240694, metrics={'train_runtime': 1094.0663, 'train_samples_per_second': 615.584, 'train_steps_per_second': 19.24, 'total_flos': 7163019700247808.0, 'train_loss': 0.28101540195240694, 'epoch': 1.62})"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    data_collator=data_collator,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets['train'],\n",
        "    eval_dataset=tokenized_datasets['validation'],\n",
        "    compute_metrics=compute_metrics, #compute_metrics1,#compute_metrics_classification,\n",
        "    callbacks = [EarlyStoppingCallback(early_stopping_patience=20)]\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
